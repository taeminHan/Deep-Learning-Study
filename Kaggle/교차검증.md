이 튜토리얼에서는 모델 성능을 더 잘 측정하기 위해 교차 검증을 사용하는 방법에 대해 알아봅니다.

#Introduce
기계 학습은 반복적인 과정이다.

사용할 예측 변수, 사용할 모형 유형, 해당 모형에 제공할 인수 등에 대한 선택 사항에 직면하게 됩니다. 지금까지 검증(또는 홀드아웃) 세트를 사용하여 모델 품질을 측정하여 데이터 중심 방식으로 이러한 선택을 수행했습니다.

그러나 이 접근법에는 몇 가지 단점이 있다. 이를 확인하려면 5000개의 행이 있는 데이터 세트를 사용한다고 가정해 보십시오. 일반적으로 데이터의 약 20%를 검증 데이터 세트 또는 1000개의 행으로 유지합니다. 그러나 이것은 모델 점수를 결정할 수 있는 임의의 기회를 남깁니다. 즉, 다른 1000행에서 부정확하더라도 모형이 1000행의 한 집합에서 잘 수행될 수 있습니다.

극단적으로 검증 집합에는 데이터 행이 하나만 있는 것을 상상할 수 있습니다. 대체 모형을 비교할 경우 단일 데이터 점에서 가장 적합한 예측은 대부분 운에 따라 결정됩니다.

일반적으로 검증 집합이 클수록 랜덤성(일명 "노이즈"라고 함)은 모델 품질을 측정하는 데 더 적으며, 더 신뢰할 수 있습니다. 안타깝게도, 우리는 훈련 데이터에서 행을 제거하여 대규모 검증 세트를 얻을 수 있을 뿐이며, 훈련 데이터 세트가 작으면 더 나쁜 모델이 된다는 것을 의미한다!

#Cross Validation 이란?

교차 검증이란?
교차 검증에서는 데이터의 여러 하위 집합에 대해 모델링 프로세스를 실행하여 모델 품질의 여러 측도를 얻는다.

예를 들어, 우리는 데이터를 전체 데이터 세트의 20% 각각 5개로 나누는 것으로 시작할 수 있다. 이 경우, 우리는 데이터를 5개의 "배"로 나누었다고 말합니다.
(For example, we could begin by dividing the data into 5 pieces, each 20% of the full dataset. In this case, we say that we have broken the data into 5 "folds".)

![img.png](img.png)

그런 다음 각 접기마다 하나의 실험을 실행합니다.

실험 1에서는 첫 번째 폴드를 검증(또는 홀드아웃) 세트로 사용하고 다른 모든 것은 훈련 데이터로 사용한다. 

20% 홀드아웃 세트를 기반으로 모델 품질을 측정할 수 있습니다.

실험 2에서는 두 번째 폴드의 데이터를 보류하고(그리고 두 번째 폴드를 제외한 모든 데이터를 모델을 훈련하는 데 사용한다. 

그런 다음 홀드아웃 집합을 사용하여 모형 품질의 두 번째 추정치를 얻습니다.

이 프로세스를 반복하여 모든 접기를 홀드아웃 세트로 사용합니다. 

이를 종합하면, 데이터의 100%가 어느 시점에서는 홀드아웃으로 사용되며, 우리는 (모든 행을 동시에 사용하지 않더라도) 

데이터 세트의 모든 행을 기반으로 하는 모델 품질을 측정하게 된다.

# 교차 검증은 언제 사용해야 할까?

교차 검증을 사용하면 모형 품질을 보다 정확하게 측정할 수 있으며, 모형 결정을 많이 내릴 경우 특히 중요합니다. 

그러나 여러 모형(각 접기마다 하나씩)을 추정하기 때문에 실행하는 데 시간이 더 오래 걸릴 수 있습니다.

그렇다면, 이러한 트레이드오프를 고려할 때, 여러분은 각 접근 방식을 언제 사용해야 할까요?

추가 계산 부담이 크지 않은 소규모 데이터 세트의 경우 교차 검증을 실행해야 합니다.

대규모 데이터 세트의 경우 단일 검증 집합으로 충분합니다. 

코드가 더 빨리 실행되므로 데이터를 충분히 확보하여 일부 코드를 다시 사용할 필요가 없습니다.


큰 데이터 세트와 작은 데이터 세트를 구성하는 항목에 대한 간단한 임계값은 없습니다. 

그러나 모델을 실행하는 데 몇 분 또는 그 이하의 시간이 걸린다면 교차 검증으로 전환하는 것이 좋습니다.

또는 교차 검증을 실행하여 각 실험의 점수가 가까운지 확인할 수 있습니다. 

각 실험이 동일한 결과를 산출하는 경우 단일 검증 집합으로 충분할 수 있습니다.

# Example

이전 튜토리얼과 동일한 데이터로 작업하겠습니다. 입력 데이터는 X로, 출력 데이터는 y로 로드합니다.

```python
import pandas as pd

# Read the data
data = pd.read_csv('../input/melbourne-housing-snapshot/melb_data.csv')

# Select subset of predictors
cols_to_use = ['Rooms', 'Distance', 'Landsize', 'BuildingArea', 'YearBuilt']
X = data[cols_to_use]

# Select target
y = data.Price
```


그런 다음, 우리는 임퓨터를 사용하여 결측값을 채우고 랜덤 포레스트 모델을 사용하여 예측하는 파이프라인을 정의한다.

파이프라인 없이 교차 검증을 수행하는 것은 가능하지만, 매우 어렵습니다! 파이프라인을 사용하면 코드가 상당히 간단해집니다.

    
```python
from sklearn.ensemble import RandomForestRegressor
from sklearn.pipeline import Pipeline
from sklearn.impute import SimpleImputer

my_pipeline = Pipeline(steps=[('preprocessor', SimpleImputer()),
                              ('model', RandomForestRegressor(n_estimators=50,
                                                              random_state=0))])
```
우리는 scikit-learn에서 cross_val_score() 함수로 교차 검증 점수를 얻는다. 
우리는 cv 파라미터로 접는 횟수를 설정합니다.
```python
from sklearn.model_selection import cross_val_score

# Multiply by -1 since sklearn calculates *negative* MAE
scores = -1 * cross_val_score(my_pipeline, X, y,
                              cv=5,
                              scoring='neg_mean_absolute_error')

print("MAE scores:\n", scores)
```

점수 매기기 매개 변수는 보고할 모델 품질의 측도를 선택합니다. 
이 경우 우리는 음의 평균 절대 오차(MAE)를 선택했습니다. 
scikit-learn에 대한 문서에는 옵션 목록이 표시됩니다.

우리가 부정적인 MAE를 명시하는 것은 약간 놀랍다. 
Scikit-learn에는 모든 메트릭이 정의되므로 높은 수가 더 나은 규칙이 있습니다. 
부정적인 MAE는 다른 곳에서는 거의 찾아볼 수 없지만, 여기서 부정적인 MAE를 사용하면 관습과 일치할 수 있다.

우리는 일반적으로 대체 모델을 비교할 수 있는 모델 품질의 단일 측도를 원한다. 
그래서 우리는 실험을 통해 평균을 구합니다.

```python
print("Average MAE score (across experiments):")
print(scores.mean())
```

#결론

교차 검증을 사용하면 모델 품질을 훨씬 더 잘 측정할 수 있으며 코드를 정리할 경우 추가적인 이점이 있습니다. 
더 이상 별도의 교육 및 검증 세트를 추적할 필요가 없습니다. 
특히 소규모 데이터셋의 경우 이 기능이 상당히 향상되었습니다